{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise objectives:\n",
    "- Convert üî† words to üî¢ vector representations thanks to embeddings\n",
    "- Discover the powerful Word2Vec algorithm\n",
    "\n",
    "<hr>\n",
    "\n",
    "_Embeddings_ are representations of words using vectors. These embeddings can be learned within a Neural Network. But it can take time to converge. Another option is to learn them as a first step. Then, use them directly to feed the word representations into a Recurrent Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è Run this cell and make sure the version of üìö [Gensim - Word2Vec](https://radimrehurek.com/gensim/auto_examples/index.html) you are using is ‚â• 4.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.10.0\r\n",
      "tensorflow-datasets==4.6.0\r\n",
      "tensorflow-estimator==2.10.0\r\n",
      "tensorflow-io-gcs-filesystem==0.27.0\r\n",
      "tensorflow-metadata==1.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides many datasets, among which is the IMDB dataset üé¨:\n",
    "- It is comprised of sentences that are ***movie reviews***. \n",
    "- Each of these reviews is related to a score given by the reviewer.\n",
    "\n",
    "‚ùì **Question** ‚ùì First of all, let's load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "‚ö†Ô∏è **Warning** ‚ö†Ô∏è The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer can handle it. Otherwise, rerun with a lower number.  \n",
    "\n",
    "‚ö†Ô∏è **DISCLAIMER** ‚ö†Ô∏è **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 15:39:22.071815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2253cb154954588985bea9411aaa199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e76ab8c597461cb26a92f885af99f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMIXDWI/imdb_reviews-train.tfrecord*...‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMIXDWI/imdb_reviews-test.tfrecord*...:‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteMIXDWI/imdb_reviews-unsupervised.tfrec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 15:40:45.470461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Embeddings in the previous challenge</u></b>:\n",
    "\n",
    "In the previous exercise, we jointly learned a representation for the words, and fed this representation to a RNN, as shown down below üëá: \n",
    "\n",
    "<img src=\"layers_embedding.png\" width=\"400px\" />\n",
    "\n",
    "However, this increases the number of parameters to learn, which slows down and increases the difficulty of convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Embeddings in the current challenge</u></b>:\n",
    "\n",
    "For this reason, we will separate the steps of learning the word representation and feeding it into a RNN. As shown here: \n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "We will learn the embedding with Word2Vec.\n",
    "\n",
    "The drawback is indeed that the learned embeddings are not _specifically_ designed for our task. However, learning them independently of the task at hand (sentiment analysis) has some advantages: \n",
    "- it is very fast to do in general (with Word2Vec)\n",
    "- the representation learned by Word2Vec is still meaningful \n",
    "- the convergence of the RNN alone will be easier and faster\n",
    "\n",
    "So let's learn an embedding with Word2Vec and see how meaningful it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Word2Vec to embed the words of our sentences. Word2Vec will be able to convert each word to a fixed-size vectorial representation.\n",
    "\n",
    "For instance, we will have:\n",
    "- üê∂ _dog_ $\\rightarrow$ [0.1, -0.3, 0.8]\n",
    "- üê± _cat_ $\\rightarrow$ [-1.1, 2.3, 0.7]\n",
    "- üçè _apple_ $\\rightarrow$ [3.1, 0.9, -4.7]\n",
    "\n",
    "Here, your embedding space is of size 3.\n",
    "\n",
    "***What is a \"good\" numerical representation of words?***\n",
    "\n",
    "- ***Words with close meanings should be geometrically close in your embedding space!***\n",
    "\n",
    "    - Look at the following example which represents a bi-dimensional embedding space.\n",
    "\n",
    "![Embedding](word_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Let's run Word2Vec! \n",
    "\n",
    "[üìö **Gensim**](https://radimrehurek.com/gensim/)  is a great Python package that makes the use of the Word2Vec algorithm easy to implement, fast and accurate (which is not an easy task!).\n",
    "\n",
    "1. The following code imports Word2Vec from Gensim. \n",
    "\n",
    "2. The second line learns the embedding representation of the words thanks to the sentences in `X_train`. \n",
    "3. The third line stores the words and their trained embeddings in `wv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train)\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the embedded representation of some words.\n",
    "\n",
    "You can use `wv` as a dictionary.\n",
    "For instance, `wv['dog']` will return a representation of `dog` in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Try different words - especially, try non-existing words to see that they don't have any representation (which is perfectly normal as their representation was not learned). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'dog': [-0.08555823  0.17339347 -0.08181087  0.21108234 -0.02032751 -0.32627067\n",
      "  0.04378813  0.5240822  -0.20800665 -0.15857576 -0.00494731 -0.28844658\n",
      "  0.0585472   0.15533428  0.02426083 -0.23761147  0.17209926 -0.22785889\n",
      " -0.07049159 -0.4302882   0.19429903  0.05993873  0.31784904 -0.15720111\n",
      " -0.00408607  0.01088329 -0.18901946 -0.10262786 -0.20828311 -0.01181161\n",
      "  0.1945869   0.00372149  0.13275695 -0.32653853 -0.1841536   0.23749484\n",
      "  0.08341638 -0.10567769 -0.26443875 -0.34345752 -0.14148036 -0.28053665\n",
      " -0.23240499  0.19585615  0.27611262 -0.04167099 -0.1541255  -0.13019587\n",
      "  0.20721921  0.18885952  0.05336806 -0.18910179 -0.27054396 -0.05237565\n",
      " -0.13248296  0.14661637  0.11713735  0.01220182 -0.21446398  0.03165093\n",
      "  0.12909262 -0.0588934   0.04267502  0.10239565 -0.21414989  0.31340128\n",
      " -0.0568453   0.21128896 -0.33794597  0.20631337 -0.19552083  0.23485897\n",
      "  0.24956402 -0.2103832   0.38875028  0.06379774  0.02877027  0.11462229\n",
      " -0.19564226 -0.05439083 -0.23980992 -0.190706   -0.25960898  0.22304597\n",
      " -0.1507343  -0.21588016  0.0216859   0.25142092  0.31066984  0.06961115\n",
      "  0.18081208  0.28268644  0.05913593  0.10549433  0.43874103  0.12532257\n",
      "  0.22135901 -0.11346225  0.07493441 -0.02261302]\n",
      "Embedding for 'cat': [-0.17335851  0.20141485 -0.27831593  0.38445565 -0.06154018 -0.24567883\n",
      "  0.10520767  0.7227753  -0.18573824 -0.25677988  0.00254188 -0.3797222\n",
      "  0.00612915  0.15634313 -0.04906664 -0.21200822  0.2308948  -0.21874824\n",
      " -0.06150437 -0.42286396  0.13205916  0.08963963  0.2635282  -0.21562928\n",
      " -0.0568283   0.09351866 -0.28102884 -0.12275279 -0.3075354   0.04618228\n",
      "  0.15159833 -0.04220894  0.15413012 -0.54006046 -0.32279408  0.24421084\n",
      "  0.19390889 -0.09977641 -0.32152978 -0.22086187 -0.17592685 -0.33521995\n",
      " -0.46716642  0.30246162  0.43229187 -0.07826014 -0.12377924 -0.107114\n",
      "  0.48080584  0.22100638  0.1451079  -0.3224598  -0.19238594 -0.03014484\n",
      " -0.24926665  0.24567679  0.2528645  -0.11407819 -0.06846737  0.026012\n",
      "  0.2659168  -0.05141317  0.06516467  0.14543943 -0.11094008  0.27572954\n",
      " -0.05739573  0.35140097 -0.48718166  0.3056232  -0.34595084  0.17372437\n",
      "  0.45427084 -0.3515888   0.44545755  0.17809512 -0.04888991  0.25916114\n",
      " -0.23939283 -0.1046005  -0.4464256  -0.31098935 -0.28949803  0.27456525\n",
      " -0.1682831  -0.36145797 -0.11155365  0.35761788  0.4289344   0.09656849\n",
      "  0.16588366  0.4942321   0.09626506  0.1765429   0.634211    0.10955355\n",
      "  0.27619067 -0.02292316  0.15156874  0.1320134 ]\n",
      "No embedding found for 'apple'\n",
      "No embedding found for 'non_existing_word'\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming you've already trained the Word2Vec model with X_train as shown previously\n",
    "word2vec = Word2Vec(sentences=X_train)\n",
    "wv = word2vec.wv\n",
    "\n",
    "# Examples of words to check their embeddings\n",
    "words_to_check = [\"dog\", \"cat\", \"apple\", \"non_existing_word\"]\n",
    "\n",
    "for word in words_to_check:\n",
    "    try:\n",
    "        print(f\"Embedding for '{word}': {wv[word]}\")\n",
    "    except KeyError:\n",
    "        print(f\"No embedding found for '{word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì What is the size of each word representation, and therefore, what is the size of the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding space: 100\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'dog' is in the vocabulary and its embedding is already printed\n",
    "embedding_size_for_dog = len(wv['dog'])\n",
    "\n",
    "print(\"Size of the embedding space:\", embedding_size_for_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßê How do we know whether this embedding make any sense or not? \n",
    "\n",
    "üí° To investigate this question, we will check that words with a close meaning have close representations. \n",
    "\n",
    "üëâ Let's use the [**`Word2Vec.wv.most_similar`**](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method that, given an input word, displays the \"closest\" words in the embedding space. If the embedding is well done, then words with similar meanings will have similar representation in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Try out the `most_similar` method on different words. \n",
    "\n",
    "üßëüèø‚Äçüè´ The quality of the closeness will depend on the quality of your embedding, and thus, depend on the number of sentences that you have loaded and from which you create your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'dog':\n",
      "  research (similarity: 0.9912723898887634)\n",
      "  man's (similarity: 0.9891747236251831)\n",
      "  intention (similarity: 0.9889662861824036)\n",
      "  political (similarity: 0.9889214634895325)\n",
      "  greek (similarity: 0.9880340099334717)\n",
      "  babies (similarity: 0.9876160621643066)\n",
      "  unforgettable (similarity: 0.9876061081886292)\n",
      "  streisand (similarity: 0.9872338175773621)\n",
      "  experiment (similarity: 0.9867767095565796)\n",
      "  nuclear (similarity: 0.9863289594650269)\n",
      "\n",
      "Words most similar to 'movie':\n",
      "  film (similarity: 0.9661290645599365)\n",
      "  show (similarity: 0.843681812286377)\n",
      "  thing (similarity: 0.8244465589523315)\n",
      "  ending (similarity: 0.7924070358276367)\n",
      "  series (similarity: 0.7861598134040833)\n",
      "  sequel (similarity: 0.7814816832542419)\n",
      "  flick (similarity: 0.7599536180496216)\n",
      "  book (similarity: 0.7547544240951538)\n",
      "  fun (similarity: 0.7492166757583618)\n",
      "  watching (similarity: 0.7457419037818909)\n",
      "\n",
      "Words most similar to 'happy':\n",
      "  saying (similarity: 0.9677345156669617)\n",
      "  normally (similarity: 0.9667773246765137)\n",
      "  difficult (similarity: 0.9667650461196899)\n",
      "  interested (similarity: 0.9623377919197083)\n",
      "  complaining (similarity: 0.9593342542648315)\n",
      "  shock (similarity: 0.9575488567352295)\n",
      "  deserve (similarity: 0.9562944173812866)\n",
      "  impossible (similarity: 0.9549211859703064)\n",
      "  hey (similarity: 0.9543712735176086)\n",
      "  painful (similarity: 0.9536522626876831)\n",
      "\n",
      "Words most similar to 'sad':\n",
      "  okay (similarity: 0.9601770639419556)\n",
      "  stupid (similarity: 0.9600309729576111)\n",
      "  sucks (similarity: 0.9588154554367065)\n",
      "  dated (similarity: 0.9587685465812683)\n",
      "  damn (similarity: 0.9524877071380615)\n",
      "  curious (similarity: 0.952481210231781)\n",
      "  simply (similarity: 0.9491908550262451)\n",
      "  positive (similarity: 0.9454848766326904)\n",
      "  seriously (similarity: 0.9440926909446716)\n",
      "  cool (similarity: 0.9424951672554016)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples of words to check for similarity\n",
    "words_to_check_similarity = [\"dog\", \"movie\", \"happy\", \"sad\"]\n",
    "\n",
    "for word in words_to_check_similarity:\n",
    "    try:\n",
    "        similar_words = wv.most_similar(word)\n",
    "        print(f\"Words most similar to '{word}':\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word} (similarity: {similarity})\")\n",
    "        print()\n",
    "    except KeyError:\n",
    "        print(f\"No similar words found for '{word}' - it might not be in the vocabulary.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Similarly to `most_similar` used on words directly, we can use [**`similar_by_vector`**](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similar_by_vector) on vectors to do the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to the vector of 'dog':\n",
      "  dog (similarity: 0.9999999403953552)\n",
      "  research (similarity: 0.9912723898887634)\n",
      "  man's (similarity: 0.9891747236251831)\n",
      "  intention (similarity: 0.9889662861824036)\n",
      "  political (similarity: 0.9889214634895325)\n",
      "  greek (similarity: 0.9880339503288269)\n",
      "  babies (similarity: 0.9876160621643066)\n",
      "  unforgettable (similarity: 0.9876061081886292)\n",
      "  streisand (similarity: 0.9872338175773621)\n",
      "  experiment (similarity: 0.9867767095565796)\n"
     ]
    }
   ],
   "source": [
    "# Example: Using the vector representation of 'dog' to find similar words\n",
    "dog_vector = wv['dog']\n",
    "\n",
    "# Find words most similar to the vector of 'dog'\n",
    "similar_words_to_vector = wv.similar_by_vector(dog_vector)\n",
    "\n",
    "print(\"Words most similar to the vector of 'dog':\")\n",
    "for similar_word, similarity in similar_words_to_vector:\n",
    "    print(f\"  {similar_word} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic on words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform some mathematical operations on words, i.e. on their vector representations!\n",
    "\n",
    "As any learned word is represented as a vector, you can do basic arithmetic operations, such as:\n",
    "\n",
    "$$W2V(good) - W2V(bad)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Do this mathematical operation and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of W2V('good') - W2V('bad'): [-0.33491522 -0.39934835  0.21403015  0.3361305   0.0590021  -0.1518879\n",
      " -0.08099687  0.04494332 -0.1558781  -0.25252956  0.10859609  0.09595746\n",
      "  0.05276501 -0.08170962 -0.08264568  0.4310434  -0.3747596   0.29057598\n",
      " -0.1192195   0.07043684 -0.35550702  0.10595404 -0.2046206   0.07378142\n",
      " -0.1618473  -0.10472789 -0.1766685   0.7271621  -0.01663208 -0.83334863\n",
      " -0.15421146  0.24641627  0.03751349  0.30387425  0.0009017  -0.4853022\n",
      "  0.25204936 -0.13798207 -0.5347204   0.7624682  -0.11195929 -0.16153759\n",
      " -0.32178906 -0.34848517  0.5733923  -0.14840174  0.4846446  -0.10596311\n",
      "  0.05075854  0.553197   -0.07314749  0.01497471 -0.4091641   0.12564003\n",
      "  0.11956668  0.05222023 -0.26890734  0.12221307  0.1411688   0.68235123\n",
      "  0.03219012  0.31542993 -0.15815812 -0.42742652  0.15388364 -0.00543487\n",
      " -0.53245044 -0.1607559   0.48167282  0.56848186 -0.743979   -0.33208567\n",
      "  0.3595556   0.16524208 -0.17978448  0.04131694 -0.00478932 -0.12449896\n",
      "  0.28178966  0.04666227 -0.01040667 -0.115798   -0.5057003   0.35527337\n",
      " -0.5657717   0.36784047 -0.22459137  0.08380124  0.18392095 -0.7046099\n",
      " -0.42314053  0.10551429  0.2627608   0.5952112  -0.34057724 -0.5369276\n",
      "  0.3277098   0.36676544 -0.6735344  -0.01603425]\n"
     ]
    }
   ],
   "source": [
    "# Perform the operation: vector(\"good\") - vector(\"bad\")\n",
    "vector_good = wv['good']\n",
    "vector_bad = wv['bad']\n",
    "result_vector = vector_good - vector_bad\n",
    "\n",
    "print(\"Result of W2V('good') - W2V('bad'):\", result_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine for a second that the following equality holds true:\n",
    "\n",
    "$$W2V(good) - W2V(bad) = W2V(nice) - W2V(stupid)$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$W2V(good) - W2V(bad) + W2V(stupid) = W2V(nice)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Let's, just for fun (as it would be bold of us to think that this equality holds true ...), do the operation $W2V(good) - W2V(bad) + W2V(stupid)$ and store it in a `res` variable (which will be a vector of size 100 that you can print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of W2V('good') - W2V('bad') + W2V('stupid'): [-0.07430738 -0.46666953  0.3914961  -0.02891597  0.14834842 -0.7675248\n",
      " -0.26757705  0.47757328 -0.5598122  -0.52179074 -0.11470663 -0.31723464\n",
      "  0.07442862  0.16343768  0.08463763  0.19721806 -0.07568207 -0.23376471\n",
      " -0.3592999  -0.7580217   0.11102498  0.26100138  0.26174116  0.11038011\n",
      " -0.3222077  -0.27706707 -0.3240376   0.50751185 -0.24323463 -0.7517084\n",
      "  0.43493748  0.26979607  0.32388505  0.04884326 -0.37326285  0.3248775\n",
      "  0.2095495   0.04816389 -0.69738847 -0.17218566 -0.05783308 -0.62679446\n",
      " -0.02515575  0.1850509   0.7196395  -0.06600647  0.16997868 -0.49373916\n",
      " -0.06357201  0.636631    0.06898028 -0.32254255 -0.53596157  0.17868918\n",
      "  0.10893975  0.40474984 -0.05074216  0.06358797 -0.40205082  0.25674167\n",
      "  0.10113913  0.09353857  0.23224327 -0.16455022 -0.5994894   0.5754331\n",
      " -0.48052624 -0.11690301  0.11707014  0.7247374  -0.6012769  -0.03476503\n",
      "  0.54854554  0.32502759  0.6474283   0.18940568  0.03015812 -0.21185443\n",
      " -0.2814626   0.21519735  0.04119225 -0.20032895 -1.0616672   0.8955851\n",
      " -0.55320895  0.21421777  0.27287763  0.12885313  0.32832432 -0.18517995\n",
      "  0.3017938   0.34723285  0.36106944  0.57282907  0.5062136   0.07541287\n",
      "  0.56940514 -0.27254015 -0.45326835 -0.57835   ]\n"
     ]
    }
   ],
   "source": [
    "# Perform the operation: W2V(\"good\") - W2V(\"bad\") + W2V(\"stupid\")\n",
    "vector_good = wv['good']\n",
    "vector_bad = wv['bad']\n",
    "vector_stupid = wv['stupid']\n",
    "\n",
    "res = vector_good - vector_bad + vector_stupid\n",
    "\n",
    "print(\"Result of W2V('good') - W2V('bad') + W2V('stupid'):\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said earlier, that for any vector it is possible to see the closest vectors in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at the closest vectors of `res`\n",
    "\n",
    "üí° _Hint_: `similar_by_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to the result vector:\n",
      "  given (similarity: 0.7771015763282776)\n",
      "  nice (similarity: 0.7645534873008728)\n",
      "  always (similarity: 0.7637762427330017)\n",
      "  spark (similarity: 0.7472996711730957)\n",
      "  posted (similarity: 0.7446976900100708)\n",
      "  although (similarity: 0.7440087199211121)\n",
      "  used (similarity: 0.7399207353591919)\n",
      "  fair (similarity: 0.7395617961883545)\n",
      "  good (similarity: 0.733678936958313)\n",
      "  decent (similarity: 0.731594979763031)\n"
     ]
    }
   ],
   "source": [
    "# Find the words most similar to the vector stored in 'res'\n",
    "similar_words_to_res = wv.similar_by_vector(res)\n",
    "\n",
    "print(\"Words most similar to the result vector:\")\n",
    "for similar_word, similarity in similar_words_to_res:\n",
    "    print(f\"  {similar_word} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredible right! You can do arithmetic operations on words!\n",
    "\n",
    "‚ùì **Question** ‚ùì You can try on arithmetic such as \n",
    "\n",
    "$$W2V(Boy) - W2V(Girl) = W2V(Man) - W2V(Woman)$$\n",
    "\n",
    "or \n",
    "\n",
    "$$W2V(Queen) - W2V(King) = W2V(actress) - W2V(actor)$$\n",
    "\n",
    "‚ùó **Remark** ‚ùó You will probably see that the results are not perfect. But don't forget that you trained your model on a very small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to the vector of 'Boy' - 'Girl':\n",
      "  10 (similarity: 0.6828743815422058)\n",
      "  2 (similarity: 0.6552428603172302)\n",
      "  1 (similarity: 0.6201376914978027)\n",
      "  3 (similarity: 0.6161310076713562)\n",
      "  minutes (similarity: 0.5957394242286682)\n",
      "  5 (similarity: 0.5681634545326233)\n",
      "  tv (similarity: 0.5670452117919922)\n",
      "  my (similarity: 0.5658004879951477)\n",
      "  worst (similarity: 0.5565696954727173)\n",
      "  4 (similarity: 0.5293103456497192)\n",
      "\n",
      "Words most similar to the vector of 'Queen' - 'King':\n",
      "  she (similarity: 0.18317453563213348)\n",
      "  he (similarity: 0.13897664844989777)\n",
      "  who (similarity: 0.09356987476348877)\n",
      "  never (similarity: 0.08082055300474167)\n",
      "  what (similarity: 0.05337127670645714)\n",
      "  when (similarity: 0.04753798991441727)\n",
      "  why (similarity: 0.040311217308044434)\n",
      "  her (similarity: 0.035780299454927444)\n",
      "  him (similarity: 0.034226350486278534)\n",
      "  that (similarity: 0.013694081455469131)\n"
     ]
    }
   ],
   "source": [
    "# Operation: W2V(\"Boy\") - W2V(\"Girl\")\n",
    "vector_boy = wv['boy']\n",
    "vector_girl = wv['girl']\n",
    "result_vector_boy_girl = vector_boy - vector_girl\n",
    "\n",
    "# Find the words most similar to the result vector for \"Boy\" - \"Girl\"\n",
    "similar_words_boy_girl = wv.similar_by_vector(result_vector_boy_girl)\n",
    "\n",
    "print(\"Words most similar to the vector of 'Boy' - 'Girl':\")\n",
    "for similar_word, similarity in similar_words_boy_girl:\n",
    "    print(f\"  {similar_word} (similarity: {similarity})\")\n",
    "print()\n",
    "\n",
    "# Operation: W2V(\"Queen\") - W2V(\"King\")\n",
    "vector_queen = wv['queen']\n",
    "vector_king = wv['king']\n",
    "result_vector_queen_king = vector_queen - vector_king\n",
    "\n",
    "# Find the words most similar to the result vector for \"Queen\" - \"King\"\n",
    "similar_words_queen_king = wv.similar_by_vector(result_vector_queen_king)\n",
    "\n",
    "print(\"Words most similar to the vector of 'Queen' - 'King':\")\n",
    "for similar_word, similarity in similar_words_queen_king:\n",
    "    print(f\"  {similar_word} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i>Some notes about Word2Vec as an internal Neural Network</i></u>:\n",
    "\n",
    "You might wonder where does this magic comes from (at quite a low price, you just ran a line of code on a very small corpus and it was trained within few minutes). The magic comes from the way Word2Vec is trained. The details are quite complex, but you can remember that Word2vec, in `word2vec = Word2Vec(sentences=X_train)`, actually trains a internal neural network (that you don't see).  \n",
    "\n",
    "In a nutshell, this internal neural network predicts a word from the surroundings words in a sentences. Hence, it splits the original sentences, then for each split it chooses some words as inputs $X$ and a word as the output $y$ which it tries to predict, using the embedding space.\n",
    "\n",
    "And as with any neural network, Word2Vec has some hyperparameters. Let's play with some of these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì The first important hyperparameter is the `vector_size` argument. It corresponds to the size of the embedding space. Learn a new `word2vec_2` model, still trained on the `X_train`, but with a smaller or higher `vector_size`.\n",
    "\n",
    "Verify on some words that the embedding size is the one you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'dog' with vector size 50: [ 0.04225151  0.00188194 -0.04602172  0.08343252 -0.07825615 -0.44203973\n",
      "  0.32264626  0.62627864 -0.74150693 -0.22374304 -0.12112553 -0.5285265\n",
      " -0.13321267  0.12990339  0.0505675   0.2655882   0.25878403  0.16548042\n",
      " -0.59291196 -0.40122205  0.0933851   0.20387961  0.5519996  -0.23980533\n",
      "  0.31096533  0.05027333 -0.24279663  0.17375876 -0.31118268  0.11954911\n",
      " -0.06141366 -0.26943132 -0.25731024  0.03257808 -0.16933754  0.13686617\n",
      "  0.31253004  0.05351512  0.09517422 -0.21734266  0.3185163  -0.04427274\n",
      " -0.09767125  0.23966514  0.49648875  0.12202334  0.13892101 -0.5442693\n",
      "  0.3798602   0.27710918]\n",
      "Size of embedding: 50\n",
      "\n",
      "Embedding for 'movie' with vector size 50: [-0.3032787   0.73370713 -0.02492724 -1.0187318   0.6587918  -1.1219642\n",
      "  0.67373866  1.9380773   0.5520067  -1.0656253   0.0099706  -1.1125219\n",
      "  3.1161475   0.5385937  -1.6240096   1.3214861   1.182277    1.7344649\n",
      " -2.1508431  -0.2676459  -0.5228351   0.28891042  0.80866903  2.34802\n",
      "  0.17163818 -0.63260347 -1.0132544   0.16132703 -1.4965506   0.1555878\n",
      "  0.6741726  -0.04302019  2.7078998   0.35817558 -0.7967691   0.6230496\n",
      "  0.90658736 -1.261613   -1.3608341   2.1457803   1.0235301   0.17223236\n",
      "  0.39125118 -0.28286645  2.7899458   1.0959358   1.0168352   2.5335708\n",
      "  1.2170497  -0.01296276]\n",
      "Size of embedding: 50\n",
      "\n",
      "Embedding for 'happy' with vector size 50: [-0.12325107 -0.05588197 -0.13929468 -0.01544012 -0.04460909 -0.41522804\n",
      "  0.30861422  0.58788174 -0.67119074 -0.33788392  0.09621177 -0.54518056\n",
      "  0.20887579  0.1394753  -0.1670682   0.28138843  0.51197463  0.2918199\n",
      " -0.8312411  -0.4374658  -0.01843407  0.32586062  0.57151324  0.08120605\n",
      "  0.17296313  0.14432997 -0.2315481   0.03442743 -0.28122178  0.1934774\n",
      "  0.09256791 -0.30336052 -0.10698506 -0.03804207 -0.29880103  0.25985348\n",
      "  0.2645147   0.1461869  -0.04674223  0.12689088  0.51054037 -0.07023454\n",
      " -0.02560697  0.11790903  0.6431681   0.16622329  0.29645446 -0.09618898\n",
      "  0.48646113  0.2455479 ]\n",
      "Size of embedding: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Choose a new vector size (e.g., 20, 50, or 100)\n",
    "new_vector_size = 50  # Example value, you can adjust this\n",
    "\n",
    "# Train a new Word2Vec model with the new vector size\n",
    "word2vec_2 = Word2Vec(sentences=X_train, vector_size=new_vector_size)\n",
    "\n",
    "# Access the word vectors\n",
    "wv_2 = word2vec_2.wv\n",
    "\n",
    "# Examples of words to check their new embeddings\n",
    "words_to_check = [\"dog\", \"movie\", \"happy\"]\n",
    "\n",
    "for word in words_to_check:\n",
    "    try:\n",
    "        print(f\"Embedding for '{word}' with vector size {new_vector_size}: {wv_2[word]}\")\n",
    "        print(f\"Size of embedding: {len(wv_2[word])}\")  # This should match new_vector_size\n",
    "        print()\n",
    "    except KeyError:\n",
    "        print(f\"No embedding found for '{word}'\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Use the **`Word2Vec.wv.key_to_index`** attribute to display the size of the learned vocabulary. Compare it to the number of different words in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of learned vocabulary: 8006\n",
      "Number of different words in X_train: 30419\n"
     ]
    }
   ],
   "source": [
    "# Size of the learned vocabulary\n",
    "vocab_size_word2vec = len(wv_2.key_to_index)\n",
    "print(\"Size of learned vocabulary:\", vocab_size_word2vec)\n",
    "\n",
    "# Calculate the number of unique words in X_train\n",
    "unique_words_in_X_train = set(word for sentence in X_train for word in sentence)\n",
    "unique_word_count_in_X_train = len(unique_words_in_X_train)\n",
    "print(\"Number of different words in X_train:\", unique_word_count_in_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important difference between the number of words in the train sentences and in the Word2Vec vocabulary, even though it has been trained on the train sentence set. The reasons comes from the second important hyperparameter of Word2Vec:  `min_count`. \n",
    "\n",
    "`min_count` is a integer that tells you how many occurrences a given word should have to be learned in the embedding space. For instance, let's say that the word \"movie\" appears 1000 times in the corpus and \"simba\" only 2 times. If `min_count=3`, the word \"simba\" will be skipped during the training.\n",
    "\n",
    "The intention is to learn a representation of words that are sufficiently present in the corpus to have a robust embedded representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Learn a new `word2vec_3` model with a `min_count` higher than 5 (which is the default value) and a `word2vec_4` with a `min_count` smaller than 5, and then, compare the size of the vocabulary for all the different word2vecs that you have trained (you can choose any `vector_size` you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in word2vec_2 (default min_count=5): 8006\n",
      "Size of vocabulary in word2vec_3 (min_count=6): 6892\n",
      "Size of vocabulary in word2vec_4 (min_count=4): 9584\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec_3 with min_count higher than 5\n",
    "word2vec_3 = Word2Vec(sentences=X_train, vector_size=50, min_count=6)  # Example: min_count set to 6\n",
    "vocab_size_word2vec_3 = len(word2vec_3.wv.key_to_index)\n",
    "\n",
    "# Train word2vec_4 with min_count lower than 5\n",
    "word2vec_4 = Word2Vec(sentences=X_train, vector_size=50, min_count=4)  # Example: min_count set to 4\n",
    "vocab_size_word2vec_4 = len(word2vec_4.wv.key_to_index)\n",
    "\n",
    "# Print the sizes of the vocabularies\n",
    "print(\"Size of vocabulary in word2vec_2 (default min_count=5):\", vocab_size_word2vec)\n",
    "print(\"Size of vocabulary in word2vec_3 (min_count=6):\", vocab_size_word2vec_3)\n",
    "print(\"Size of vocabulary in word2vec_4 (min_count=4):\", vocab_size_word2vec_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Word2Vec has an internal neural network that is optimized based on some predictions. These predictions actually correspond to predicting a word based on surrounding words. The surroundings words are in a `window` which corresponds to the number of words taken into account. And you can train the Word2Vec with different `window` sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Train a new `word2vec_5` model with a `window` different than previously (default is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in word2vec_5 (window size = 7): 8006\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec_5 with a different window size\n",
    "new_window_size = 7  # Example: window size set to 7, but you can choose any size other than 5\n",
    "word2vec_5 = Word2Vec(sentences=X_train, vector_size=50, window=new_window_size)\n",
    "\n",
    "# Size of the vocabulary for word2vec_5\n",
    "vocab_size_word2vec_5 = len(word2vec_5.wv.key_to_index)\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(\"Size of vocabulary in word2vec_5 (window size = {}): {}\".format(new_window_size, vocab_size_word2vec_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments you have seen (`vector_size`, `min_count` and `window`) are usually the ones that you should start playing with to get a better performance for your model.\n",
    "\n",
    "But you can also look at other arguments in the [**üìö Documentation - gensim.models.word2vec.Text8Corpus**](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert our train and test set to RNN-ready datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `Word2Vec` is the first step to the overall process of feeding such a representation into a RNN, as shown here:\n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "\n",
    "\n",
    "Now, let's work on Step 2 by converting the training and test data into their vector representation to be ready to be fed in RNNs.\n",
    "\n",
    "‚ùì **Question** ‚ùì Now, write a function that, given a sentence, returns a matrix that corresponds to the embedding of the full sentence, which means that you have to embed each word one after the other and concatenate the result to output a 2D matrix (make sure that your output is a NumPy array)\n",
    "\n",
    "‚ùó **Remark** ‚ùó You will probably notice that some words you are trying to convert throw errors as they are said not to belong to the dictionary:\n",
    "\n",
    "- For the <font color=orange>test</font> set, this is understandable: <font color=orange>some words were not</font> in the <font color=blue>train</font> set and thus, their <font color=orange>embedded representation is unknown</font>\n",
    "- for the <font color=blue>train set</font>, due to `min_count` hyperparameter, not all the words have a vector representation.\n",
    "\n",
    "In any case, just skip the missing words here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = ['this', 'movie', 'is', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv.key_to_index:  # Check if the word is in the Word2Vec vocabulary\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "    \n",
    "    return np.array(embedded_sentence)\n",
    "    \n",
    "### Checks\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)  \n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Write a function that, given a list of sentences (each sentence being a list of words/strings), returns a list of embedded sentences (each sentence is a matrix). Apply this function to the train and test sentences\n",
    "\n",
    "üí° _Hint_: Use the previous function `embed_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    embedded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        if len(embedded_sentence) > 0:  # Only add non-empty embeddings\n",
    "            embedded_sentences.append(embedded_sentence)\n",
    "    return embedded_sentences\n",
    "    \n",
    "X_train_embedded = embedding(word2vec, X_train)\n",
    "X_test_embedded = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì In order to have ready-to-use data, do not forget to pad your sequences so you have tensors which can be divided into batches (of `batch_size`) during the optimization. Store the padded values in `X_train_pad` and `X_test_pad`. Do not forget the important arguments of the padding ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 100)\n",
    "assert(X_test_pad.shape[2] == 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üèÅ Congratulations, you are now able to use `Word2Vec` to embed your words :)\n",
    "\n",
    "üíæ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
